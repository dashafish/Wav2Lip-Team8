# Wav2Lip-Team8

Contributors - Riya Parasar, Carl Pittenger, Michael Slusser, Dasha Rizvanova

Wav2Lip is a project that utilizes deep learning techniques to synthesize realistic lip movements in a target video based on an input audio clip. By combining audio information with visual lip movements, Wav2Lip offers a powerful tool for various applications such as dubbed movies and video conferencing.

Link to original GitHub - [https://github.com/Rudrabha/Wav2Lip](https://github.com/Rudrabha/Wav2Lip)

Link to Project Report - [Paper](https://docs.google.com/document/d/1clDbCi-J-YRzxe7q65Yy83252pzqMmuQIzw7OW0Vz5o/edit?usp=sharing)

Link to Google Colab with Instructions how the GitHub Repo Code was Converted into the Google Colab Notebook - [Google Colab](https://colab.research.google.com/drive/1yky8Yw8TeEBtm6UBH_LNu0iF4EuVszcN#scrollTo=a-lmNnL1zscv)

# Easy Steps to Execute 
1. Use the [Google Colab](https://colab.research.google.com/drive/1yky8Yw8TeEBtm6UBH_LNu0iF4EuVszcN#scrollTo=a-lmNnL1zscv) link to run the inference code that will generate a lip-synced video 
